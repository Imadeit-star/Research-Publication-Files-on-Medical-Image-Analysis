{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2b83c24-f579-4147-8304-e637ab9cc47e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-30 13:28:36.669946: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-30 13:28:36.743313: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.8/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "# Loading Dependencies for Medical Image Analysis\n",
    "import optuna\n",
    "from tensorflow.keras.applications import ResNet101\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt # Import matplotlib.pyplot\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import torchvision.models as models\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "import torch.optim as optim  # Import the optim module\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import torchvision.transforms as transforms\n",
    "#from torch.utils.data import DataLoader, Subset\n",
    "#from torchvision.datasets import ImageFolder\n",
    "#from torchvision import datasets\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "#from torchvision import transforms\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "#import time\n",
    "import copy\n",
    "#import os\n",
    "import IPython\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "import threading\n",
    "import optuna.visualization\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import GPyOpt\n",
    "import GPy\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "from torch.optim.lr_scheduler import (\n",
    "    StepLR,\n",
    "    ReduceLROnPlateau,\n",
    "    CosineAnnealingLR,\n",
    "    CyclicLR,\n",
    "    OneCycleLR,\n",
    ") \n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc258f2c-16cc-4f54-86bf-132c1f92c850",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    filename='CNN_Model_Next_output.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    force=True  # ha notebookból futtatsz többször, ez kell\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e33d9354-e270-437a-a1a2-40eab64335d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scheduler(optimizer, scheduler_type, **kwargs): # This function defines the scheduler types\n",
    "    if scheduler_type == \"StepLR\":\n",
    "        return StepLR(optimizer, **kwargs)\n",
    "    elif scheduler_type == \"ReduceLROnPlateau\":\n",
    "        return ReduceLROnPlateau(optimizer, **kwargs)\n",
    "    elif scheduler_type == \"CosineAnnealingLR\":\n",
    "        return CosineAnnealingLR(optimizer, **kwargs)\n",
    "    elif scheduler_type == \"CyclicLR\":\n",
    "        return CyclicLR(optimizer, cycle_momentum=False, **kwargs)\n",
    "    elif scheduler_type == \"OneCycleLR\":\n",
    "        return OneCycleLR(optimizer, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler type: {scheduler_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99eaa024-b410-4d4e-8087-53958031b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Model_ResNet101_PyTorch(learning_rate, dropout_rate):\n",
    "    # Load pre-trained ResNet101\n",
    "    model = models.resnet101(weights='ResNet101_Weights.DEFAULT')\n",
    "\n",
    "    # Modify the input layer to accept 3 channels (assuming your input is RGB)\n",
    "    # model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    # Freeze all layers initially\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the desired layers for fine-tuning\n",
    "    for param in model.layer3.parameters(): # unfreeze Layer 3\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.layer4.parameters(): # unfreeze Layer 4\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for param in model.fc.parameters(): # Unfreeze the fully connected layer\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Modification of the classifier layer (fully connected layer)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 3),        # 3 output classes (adjust if you have a different number of classes)\n",
    "        nn.Dropout(dropout_rate),\n",
    "        nn.Softmax(dim=1)              # Softmax activation for multi-class classification\n",
    "    )\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    summary(model, (3, 224, 224), device = str(device))  # Print model summary (adjust input size if needed)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss() # Appropriate for multi-class classification\n",
    "\n",
    "    # Define optimizer - consider using a lower learning rate for earlier unfrozen layers if needed\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33dba8fa-391e-4cfc-ae4e-9ce997390559",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"learning_rate\":0.00045710987419654515, #optimal learning rate from hyperparameter search\n",
    "    \"dropout_rate\":0.25339643965539393,  # optimal dropout rate from hyperparameter search\n",
    "    \"batch_size\":256,  # Your optimal batch size from hyperparameter search #########################################x/sry i have to modify original vas 64\n",
    "    \"num_epochs\":83, #  optimal number of epochs from hyperparameter search\n",
    "    #\"factor\":0.7324426614572568,  #  optimal factor from hyperparameter search\n",
    "    #\"patience\":9, #  optimal patience from hyperparameter search\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92fc2788-f3bd-41ac-ba44-b10f4a0a26b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [-1, 256, 28, 28]             512\n",
      "             ReLU-81          [-1, 256, 28, 28]               0\n",
      "           Conv2d-82          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [-1, 256, 14, 14]             512\n",
      "             ReLU-84          [-1, 256, 14, 14]               0\n",
      "           Conv2d-85         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [-1, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [-1, 256, 14, 14]             512\n",
      "             ReLU-93          [-1, 256, 14, 14]               0\n",
      "           Conv2d-94          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-101          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [-1, 256, 14, 14]             512\n",
      "            ReLU-103          [-1, 256, 14, 14]               0\n",
      "          Conv2d-104          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [-1, 256, 14, 14]             512\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      "          Conv2d-107         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-121          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "          Conv2d-124          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [-1, 256, 14, 14]             512\n",
      "            ReLU-126          [-1, 256, 14, 14]               0\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-312          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-313          [-1, 512, 14, 14]               0\n",
      "          Conv2d-314            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-315            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-316            [-1, 512, 7, 7]               0\n",
      "          Conv2d-317           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-318           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-319           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-320           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-321           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-322           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-323            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-324            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-325            [-1, 512, 7, 7]               0\n",
      "          Conv2d-326            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-327            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-328            [-1, 512, 7, 7]               0\n",
      "          Conv2d-329           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-330           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-331           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-332           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-333            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-334            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-335            [-1, 512, 7, 7]               0\n",
      "          Conv2d-336            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-337            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-338            [-1, 512, 7, 7]               0\n",
      "          Conv2d-339           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-340           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-341           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-342           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-343           [-1, 2048, 1, 1]               0\n",
      "          Linear-344                    [-1, 3]           6,147\n",
      "         Dropout-345                    [-1, 3]               0\n",
      "         Softmax-346                    [-1, 3]               0\n",
      "================================================================\n",
      "Total params: 42,506,307\n",
      "Trainable params: 41,061,379\n",
      "Non-trainable params: 1,444,928\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 429.72\n",
      "Params size (MB): 162.15\n",
      "Estimated Total Size (MB): 592.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model, criterion, optimizer = Model_ResNet101_PyTorch(learning_rate=best_params[\"learning_rate\"],\n",
    "                                                      dropout_rate= best_params[\"dropout_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0283a58-0f7e-4b92-97c4-2581c0d0d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImageDataset(Dataset):\n",
    "       def __init__(self, root_dir, image_extensions, transform=None):\n",
    "           self.root_dir = root_dir\n",
    "           self.image_extensions = image_extensions\n",
    "           #self.transform = transform\n",
    "           self.image_paths = []\n",
    "           self.labels = []\n",
    "\n",
    "           for class_name in os.listdir(root_dir):  \n",
    "               class_path = os.path.join(root_dir, class_name)\n",
    "               if os.path.isdir(class_path):\n",
    "                   for filename in os.listdir(class_path):\n",
    "                       if filename.lower().endswith(image_extensions):\n",
    "                           self.image_paths.append(os.path.join(class_path, filename))\n",
    "                           self.labels.append(class_name)  # Assuming subfolder name is the label\n",
    "\n",
    "       def __len__(self):\n",
    "           return len(self.image_paths)\n",
    "\n",
    "       def __getitem__(self, idx):\n",
    "           image_path = self.image_paths[idx]\n",
    "           image = np.array(Image.open(image_path).convert('RGB'))  # Convert to NumPy array\n",
    "           label = self.labels[idx]\n",
    "           label_mapping = {'Benign': 0, 'Malignant': 1, 'Normal': 2}\n",
    "           label = label_mapping.get(label)  # Apply the label mapping here\n",
    "           #image = np.array(image)\n",
    "\n",
    "           #if self.transform:\n",
    "               #image = self.transform(image)\n",
    "\n",
    "           return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9b011f-f7a8-4aac-815a-52a4c36d66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/workspace/PetersWorkspace/Abbans/TMPFS_Medical_Images/Medical_Images/'\n",
    "#dataset_path = '/usr/bin/Medical_Images'\n",
    "image_extensions = ('.png', '.jpg', '.jpeg')\n",
    "\n",
    "dataset = MedicalImageDataset(dataset_path, image_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20675206-a9ad-4749-93ba-cd81d4aa59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset based on their split proportions\n",
    "Train_ratio = 0.70 # Ration of Training Data\n",
    "Validaton_ratio = 0.15  # Ratio of Testing Data\n",
    "Testing_ratio = 0.15 # Ration of Testing Data\n",
    "\n",
    "#torch.manual_seed(42)  # You can choose any integer as your seed\n",
    "# Calculate the sizes of the train, validation, and test sets\n",
    "Dataset_size = len(dataset) # Determines the dataset size\n",
    "#Train_size = int(Train_ratio * Dataset_size) # Determines the train dataset size\n",
    "#Validation_size = int(Validaton_ratio * Dataset_size) # Determines the validation dataset size\n",
    "Test_size = int(Testing_ratio * Dataset_size) # Determines the test dataset size\n",
    "#Test_size = Dataset_size - Train_size - Validation_size # Determines the test dataset size\n",
    "Dataset_Train_Valid = Dataset_size - Test_size # Determines the new dataset size\n",
    "\n",
    "# Split the dataset into train, validation, and test sets\n",
    " #Train_dataset, Validation_dataset, Test_dataset = torch.utils.data.random_split(dataset, [Train_size, Validation_size, Test_size])\n",
    "Dataset_New, Test_dataset = torch.utils.data.random_split(dataset, [Dataset_Train_Valid, Test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a0d4d3d-d95b-4e1a-a165-a01fb3c6331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "661\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "#print(len(Train_dataset))\n",
    "#print(len(Validation_dataset))\n",
    "#print(len(Test_dataset))\n",
    "print(len(Dataset_New))\n",
    "print(len(Test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d65a5099-0410-4b30-a418-f9830d03fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Augmentation for my Training Dataset\n",
    "#For the training dataset\n",
    "#For the training dataset\n",
    "Fundamental_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.4),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.08, 0.08), scale=(0.92, 1.08)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.85, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "442cf18e-ef23-497c-8d8f-8d4be2ab5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advanced_transform = A.Compose([\n",
    "    A.MedianBlur(blur_limit=3, p=1),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.Rotate(limit=5, p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.08, rotate_limit=12, p=0.5),\n",
    "    #A.RandomSizedCrop(min_max_height=(160, 224), size =(224, 224), p=0.5),\n",
    "    A.Resize(224, 224), # OR\n",
    "    A.ElasticTransform(alpha=0.8, sigma=40, alpha_affine=40, p = 0.5),\n",
    "    A.GridDistortion(p=0.4, distort_limit=0.4),\n",
    "    A.OpticalDistortion(p=0.05, distort_limit=0.05, shift_limit=0.5),\n",
    "    #A.GaussianBlur(blur_limit=(3, 7), p=0.5),  # Gaussian Blur applied after spatial transformations\n",
    "    # CoarseDropout(Random Erasing)\n",
    "    A.CoarseDropout(max_holes=1, max_height=32, max_width=32, min_holes=1, min_height=16,\n",
    "                    min_width=16, fill_value=0,\n",
    "        p=0.5,),\n",
    "\n",
    "    # Pixel-Level Transformations\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=0.3),\n",
    "    #A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=20, p=0.4),\n",
    "    A.CLAHE(clip_limit=3.0, tile_grid_size=(6, 6), p=0.25),  # Contrast Limited Adaptive Histogram Equalization\n",
    "\n",
    "    # Noise Injection\n",
    "    #A.GaussNoise(var_limit=(0.01, 0.05), p=0.5),\n",
    "    #A.MultiplicativeNoise(multiplier=(0.9, 1.1), elementwise=True, p=0.3),  # Speckle noise\n",
    "\n",
    "         # Blurring and Sharpening\n",
    "    #A.OneOf([A.MedianBlur(blur_limit=3, p=0.5),],p=0.1),\n",
    "\n",
    "    A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=0.5),\n",
    "\n",
    "    # Normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36730dc7-43d7-4050-8718-c86539163635",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Conversion of Images into PIL for Albumentation\n",
    "    Advanced_transform, # Application of advanced transformations\n",
    "    Fundamental_transform, # Application of fundamental transformation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd756ef-6055-443c-a8f9-d196f69d7eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Augmentation for my Validaton Dataset\n",
    "Validation_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f955f3e-006e-454d-9752-85187673de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Augmentation for the Testing Dataset\n",
    "Test_transform = transforms.Compose([\n",
    "    # Only resize and normalize for test set\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef0c5a16-61af-4ec8-ac13-1f2cee1427a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Training Model\n",
    "def Train_Model(model, criterion, optimizer, dataloaders, dataset_sizes, num_epochs, scheduler, scheduler_type):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    #best_loss = float('inf')\n",
    "    train_acc_history = []\n",
    "    train_loss_history = []\n",
    "    val_acc_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"GPU CPU? using device\",device)\n",
    "    #model.to(str(device))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        #logging.info(f'Epoch {epoch}/{num_epochs - 1}') #\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        #logging.info('-' * 10) #\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() /dataset_sizes[phase]\n",
    "\n",
    "\n",
    "            #print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model if it's the best validation accuracy so far\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "            if phase == 'train':\n",
    "               #logging.info(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}') #\n",
    "               print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "               train_acc_history.append(epoch_acc.item())\n",
    "               train_loss_history.append(epoch_loss)\n",
    "\n",
    "            elif phase == 'val':\n",
    "                #logging.info(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}') #\n",
    "                print(f'Val Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                val_acc_history.append(epoch_acc.item())\n",
    "                val_loss_history.append(epoch_loss)\n",
    "\n",
    "\n",
    "        # Scheduler step logic based on scheduler_type\n",
    "        if phase == 'val':\n",
    "            if scheduler_type == \"ReduceLROnPlateau\":\n",
    "              # Step the scheduler based on validation accuracy (negative because ReduceLROnPlateau minimizes)\n",
    "              scheduler.step(-epoch_acc)\n",
    "            elif scheduler_type in [\"CosineAnnealingLR\", \"OneCycleLR\", \"CyclicLR\"]:\n",
    "              scheduler.step()\n",
    "            elif scheduler_type == \"StepLR\":\n",
    "              if (epoch + 1) % scheduler.step_size == 0:\n",
    "                    scheduler.step()\n",
    "                    # StepLR is typically called after a certain number of epochs (step_size)\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    #logging.info(f'Training is completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s') #\n",
    "    print(f'Training is completed in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3af65d2a-6dc2-424a-a24a-6624518d8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function for Optuna: Bayesian Optimization: GPyopt\n",
    "iteration_tracker = {'count': 0} # This is used to keep track of the iteration\n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "log = open(\"cell_output_GP.log\", \"a\", buffering=1)  # line-buffered\n",
    "sys.stdout = log\n",
    "sys.stderr = log\n",
    "\n",
    "now = datetime.now()\n",
    "print(\"-------------------------------------------------------------------New Model objective running is started at : \", now, \" -------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Define a custom callback function\n",
    "def custom_callback(params, val_objective_value, scheduler_type):\n",
    "    iteration_tracker['count'] += 1\n",
    "    learning_rate = params[0,0]\n",
    "    dropout_rate = params[0,1]\n",
    "    batch_size = int(round(params[0,2]))\n",
    "    num_epochs = int(round(params[0,3]))\n",
    "    gamma1 = params[0,4]  # Extract gamma1\n",
    "\n",
    "    print(f\"Iteration {iteration_tracker['count']}:\")\n",
    "    print(f\"   Hyperparameters: lr={learning_rate}, drop={dropout_rate}, bs={batch_size}, epochs={num_epochs}, scheduler= {scheduler_type}, gamma1 = {gamma1} \")\n",
    "\n",
    "    # Printing the Scheduler specific parameters\n",
    "     # Print scheduler specific parameters:\n",
    "    # Need to retrieve the correct indices based on the defined bounds order\n",
    "    # This requires mapping parameter names to indices in the 'params' array\n",
    "    param_names = [b['name'] for b in bounds]\n",
    "    param_values = dict(zip(param_names, params[0]))\n",
    "\n",
    "\n",
    "    if scheduler_type == \"StepLR\":\n",
    "        print(f\"      StepLR parameters: step_size={int(round(param_values.get('step_size', 0)))}, gamma={param_values.get('gamma', 0.0):.6f}\")\n",
    "    elif scheduler_type == \"ReduceLROnPlateau\":\n",
    "        print(f\"      ReduceLROnPlateau parameters: factor={param_values.get('factor', 0.0):.6f}, patience={int(round(param_values.get('patience', 0)))}\")\n",
    "    elif scheduler_type == \"CosineAnnealingLR\":\n",
    "        print(f\"      CosineAnnealingLR parameters: T_max={int(round(param_values.get('T_max', 0)))}, eta_min={param_values.get('eta_min', 0.0):.6f}\")\n",
    "    elif scheduler_type == \"CyclicLR\":\n",
    "        # Retrieve mode as string\n",
    "        mode_index = int(round(param_values.get('mode', 0)))\n",
    "        mode = [\"triangular\", \"triangular2\", \"exp_range\"][mode_index] if 0 <= mode_index < 3 else \"unknown\"\n",
    "        print(f\"      CyclicLR parameters: base_lr={param_values.get('base_lr', 0.0):.6f}, max_lr={param_values.get('max_lr', 0.0):.6f}, step_size_up={int(round(param_values.get('step_size_up', 0)))}, step_size_down={int(round(param_values.get('step_size_down', 0)))}, mode={mode}\")\n",
    "    elif scheduler_type == \"OneCycleLR\":\n",
    "        print(f\"      OneCycleLR parameters: max_lr={param_values.get('max_lr', 0.0):.6f}, epochs={num_epochs}, steps_per_epoch= Calculated automatically\")\n",
    "\n",
    "    print(f\"   Validation Accuracy: {val_objective_value}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def objective_gpyopt(params, K):\n",
    "    import tensorflow as tf\n",
    "    import gc  # Add this line to import the gc module\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # Extract hyperparameters from params - Need to map names to indices\n",
    "    param_names = [b['name'] for b in bounds]\n",
    "    param_values = dict(zip(param_names, params[0]))\n",
    "\n",
    "    learning_rate = param_values['learning_rate']\n",
    "    dropout_rate = param_values['dropout_rate']\n",
    "    batch_size = int(round(param_values['batch_size'])) # Ensure batch_size is an integer\n",
    "    num_epochs = int(round(param_values['num_epochs'])) # Ensure num_epochs is an integer\n",
    "    scheduler_type_index = int(round(param_values['scheduler_type']))  # Get scheduler type index\n",
    "    scheduler_type = [\n",
    "        \"StepLR\",\n",
    "        \"ReduceLROnPlateau\",\n",
    "        \"CosineAnnealingLR\",\n",
    "        \"CyclicLR\",\n",
    "        \"OneCycleLR\",\n",
    "    ][int(scheduler_type_index)]  # Map index to scheduler type\n",
    "    gamma1 = param_values['gamma1']  # Extract gamma1\n",
    "\n",
    "    # K-fold cross-validation setup\n",
    "    fold_val_accuracies = []\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=42)\n",
    "\n",
    "    # Iterate through folds\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(Dataset_New)):\n",
    "        print(f\"Fold {fold + 1}/{K}\")\n",
    "\n",
    "        # Create train and validation subsets\n",
    "        Train_subset = Subset(Dataset_New, train_idx)\n",
    "        Val_subset = Subset(Dataset_New, val_idx)\n",
    "\n",
    "        print(\"Train_subset:\", len(Train_subset))\n",
    "        print(\"Val_subset:\", len(Val_subset))\n",
    "\n",
    "        \n",
    "        Train_subset.dataset.transform = Train_transform # Transformation for training\n",
    "        Val_subset.dataset.transform = Validation_transform # Transformaing for Validation\n",
    "\n",
    "         # Define custom_collate_fn here, inside the objective funct\n",
    "        def custom_collate_fn(batch):\n",
    "            images = []\n",
    "            labels = []\n",
    "            \n",
    "            for image, label in batch:\n",
    "                image = transforms.ToPILImage()(image)  # Convert to PIL Image\n",
    "                image = transforms.Resize((224, 224))(image)  # Add resize transform here\n",
    "                image = transforms.ToTensor()(image)  # Convert to Tensor using ToTensor       \n",
    "                images.append(image)\n",
    "                labels.append(label)\n",
    "                   \n",
    "            # Stack images into a batch tensor\n",
    "            images = torch.stack(images, dim=0)\n",
    "            labels = torch.tensor(labels)\n",
    "    \n",
    "            return images, labels\n",
    "\n",
    "        # Create data loaders for train and validation subsets\n",
    "        train_loader = DataLoader(Train_subset, batch_size=batch_size, shuffle=True, num_workers = 32, collate_fn = custom_collate_fn)\n",
    "        val_loader = DataLoader(Val_subset, batch_size=batch_size, shuffle=False, num_workers = 32, collate_fn = custom_collate_fn)\n",
    "        print(f\"Steps_Per_Epoch: {len(train_loader)}\")\n",
    "\n",
    "\n",
    "        dataloaders = {\n",
    "            'train': train_loader,\n",
    "            'val': val_loader\n",
    "        }\n",
    "\n",
    "        dataset_sizes = {\n",
    "            'train': len(Train_subset),\n",
    "            'val': len(Val_subset)\n",
    "        }\n",
    "\n",
    "        # Create a new model with the suggested hyperparameters\n",
    "        model, criterion, optimizer = Model_ResNet101_PyTorch(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "        # Create Scheduler based on scheduler_type with conditional logic within objective function\n",
    "        scheduler_kwargs = {}\n",
    "        if scheduler_type == \"StepLR\":\n",
    "            scheduler_kwargs[\"step_size\"] = int(round(param_values.get(\"step_size\", 0)))\n",
    "            scheduler_kwargs[\"gamma\"] = param_values.get(\"gamma\", 0.0)\n",
    "        elif scheduler_type == \"ReduceLROnPlateau\":\n",
    "            scheduler_kwargs[\"factor\"] = param_values.get(\"factor\", 0.0)\n",
    "            scheduler_kwargs[\"patience\"] = int(round(param_values.get(\"patience\", 0)))\n",
    "        elif scheduler_type == \"CosineAnnealingLR\":\n",
    "            scheduler_kwargs[\"T_max\"] = int(round(param_values.get(\"T_max\", 0)))\n",
    "            scheduler_kwargs[\"eta_min\"] = param_values.get(\"eta_min\", 0.0)\n",
    "        elif scheduler_type == \"CyclicLR\":\n",
    "            scheduler_kwargs[\"base_lr\"] = param_values.get(\"base_lr\", 0.0)\n",
    "            scheduler_kwargs[\"max_lr\"] = param_values.get(\"max_lr\", 0.0)\n",
    "            scheduler_kwargs[\"step_size_up\"] = int(round(param_values.get(\"step_size_up\", 0)))\n",
    "            scheduler_kwargs[\"step_size_down\"] = int(round(param_values.get(\"step_size_down\", 0)))\n",
    "            mode_index = int(round(param_values.get(\"mode\", 0)))\n",
    "            scheduler_kwargs[\"mode\"] = [\"triangular\", \"triangular2\", \"exp_range\"][mode_index] if 0 <= mode_index < 3 else \"triangular\" # Default mode if index is out of bounds\n",
    "        elif scheduler_type == \"OneCycleLR\":\n",
    "            scheduler_kwargs[\"max_lr\"] = param_values.get(\"max_lr\", 0.0)\n",
    "            scheduler_kwargs[\"epochs\"] = num_epochs\n",
    "            scheduler_kwargs[\"steps_per_epoch\"] = len(dataloaders[\"train\"]) # Calculate steps per epoch\n",
    "\n",
    "\n",
    "        # Assuming create_scheduler is defined\n",
    "        scheduler = create_scheduler(optimizer, scheduler_type, **scheduler_kwargs)\n",
    "        # Train model and get validation loss for this fold\n",
    "        _,best_acc = Train_Model(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            dataloaders,\n",
    "            dataset_sizes,\n",
    "            num_epochs=num_epochs,\n",
    "            scheduler=scheduler, scheduler_type=scheduler_type\n",
    "        )\n",
    "        fold_val_accuracies.append(best_acc)\n",
    "\n",
    "    # Calculate objective function value\n",
    "    mean_val_accuracy = sum(fold_val_accuracies) / K if K>0 else 0 # Basic mean of fold accuracies\n",
    "    val_accuracy_variance = sum([(acc - mean_val_accuracy) ** 2 for acc in fold_val_accuracies]) / K if K>0 else 0\n",
    "    objective_value = -mean_val_accuracy + gamma1 * val_accuracy_variance\n",
    "\n",
    "    # Call the callback with relevant information\n",
    "    try:\n",
    "        custom_callback(params, objective_value, scheduler_type) # Pass the final objective value to callback\n",
    "    except NameError:\n",
    "        print(\"Warning: custom_callback not defined.\")\n",
    "\n",
    "    # Clean up memory\n",
    "    del model\n",
    "    del criterion\n",
    "    del optimizer\n",
    "    del scheduler\n",
    "    del dataloaders\n",
    "    del dataset_sizes\n",
    "    del Train_subset\n",
    "    del Val_subset\n",
    "    del train_loader\n",
    "    del val_loader\n",
    "    # You might need to explicitly clear CUDA cache if using GPU\n",
    "    if torch.cuda.is_available():\n",
    "         torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    tf.keras.backend.clear_session() # Clear TensorFlow session if used elsewhere\n",
    "\n",
    "    return objective_value.item() # Converts from GPU to CPU\n",
    "\n",
    "# Define the parameter space for GPyOpt\n",
    "bounds = [\n",
    "    {'name': 'learning_rate', 'type': 'continuous', 'domain': (1e-6, 1e-2)}, # Adjusted lower range\n",
    "    {'name': 'dropout_rate', 'type': 'continuous', 'domain': (0.05, 0.5)}, # Adjusted lower bound\n",
    "    {'name': 'batch_size', 'type': 'discrete', 'domain': (16, 32, 64)},\n",
    "    {'name': 'num_epochs', 'type': 'discrete', 'domain': tuple(range(10, 101))}, # Changed to discrete\n",
    "    {'name': 'gamma1', 'type': 'continuous', 'domain': (0.001, 10.0)},  # Gamma values\n",
    "    {'name': 'scheduler_type', 'type': 'discrete', 'domain': (0, 1, 2, 3, 4)},  # Scheduler type index\n",
    "    {'name': 'factor', 'type': 'continuous', 'domain': (0.1, 0.9)},  # ReduceLROnPlateau parameters\n",
    "    {'name': 'patience', 'type': 'discrete', 'domain': (1, 10)},  # ReduceLROnPlateau parameters\n",
    "    {'name': 'step_size', 'type': 'discrete', 'domain': (1, 10)},  # StepLR parameters\n",
    "    {'name': 'gamma', 'type': 'continuous', 'domain': (0.1, 0.9)},  # StepLR parameters\n",
    "    {'name': 'T_max', 'type': 'discrete', 'domain': (1, 10)},  # CosineAnnealingLR parameters\n",
    "    {'name': 'eta_min', 'type': 'continuous', 'domain': (1e-6, 1e-4)},  # CosineAnnealingLR parameters (Adjusted lower range)\n",
    "    {'name': 'base_lr', 'type': 'continuous', 'domain': (1e-6, 1e-4)},  # CyclicLR parameters (Adjusted lower range)\n",
    "    {'name': 'max_lr', 'type': 'continuous', 'domain': (1e-4, 1e-1)},  # CyclicLR, OneCycleLR parameters (Adjusted lower range)\n",
    "    {'name': 'step_size_up', 'type': 'discrete', 'domain': (1, 10)},  # CyclicLR parameters\n",
    "    {'name': 'step_size_down', 'type': 'discrete', 'domain': (1, 10)},  # CyclicLR parameters\n",
    "    {'name': 'mode', 'type': 'discrete', 'domain': (0, 1, 2)},  # CyclicLR mode index\n",
    "    # K is fixed for this optimization run\n",
    "]\n",
    "K = 7 # Change the value of K here for after the training is completed\n",
    "\n",
    "# Optimization setup with GPyOpt\n",
    "# Reset iteration tracker before starting a new optimization run\n",
    "iteration_tracker = {'count': 0}\n",
    "\n",
    "optimizer = GPyOpt.methods.BayesianOptimization(\n",
    "    f=lambda x: objective_gpyopt(x, K),  # objective function with K passed\n",
    "    domain=bounds,      # Parameter space\n",
    "    acquisition_type='EI',    # Expected Improvement\n",
    "    exact_feval = False\n",
    ")\n",
    "# Run the optimization\n",
    "max_iter = 50 # Maximum number of iterations\n",
    "optimizer.run_optimization(max_iter=max_iter, verbosity= False)\n",
    "\n",
    "# Print the results\n",
    "print(\"Best Hyperparameters:\", optimizer.x_opt)\n",
    "print(\"Best Validation Metric (to be minimized):\", optimizer.fx_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
